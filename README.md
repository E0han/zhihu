zhihu


Register the zhihu.com robot accounts automatically


python3 知乎批量注册爬虫，一点一点造轮子

知乎用户批量注册，主要使用10 minutes email 邮箱作为验证码接受口，爬虫伪api已在开发，还没有完成，主注册函数其实已经写好了，但是昨天晚上手工注册了3个小号，今天不知道为什么一直提示不能用邮箱注册了，即使我删了cookie，感觉像是服务端的问题，耐心等待开放邮箱注册，手机号注册可没免费平台伪造手机号, 注册中涉及验证码是采用下载到本地人工手动输入验证码实现的，原谅我懒得研究写轮子实现自动识别验证码，还不如人工来的快。由于知乎还没有开放邮箱注册，故暂不上传主注册函数

感觉写这些也没什么人看，如果你因为自己的项目在搜索github的时候恰好看到了我的这个项目，如果你感兴趣或者有不会使用的地方，欢迎联系我：
邮箱：**0han@protonmail.com**
知乎：@**0han**

=========
######2017.5.8
已完全上传代码，可以直接操错，但目前知乎仍未开放邮箱，即使我ip是海外......

=========
######2017.4.29 note
今天上传可以直接调用的**ip代理自动获取**，也是爬虫，从快代理网站上爬下来的，来源于我的另一个项目，经过修改很自信可以直接装备给任何爬虫使用，使用方法如下:


`#proxy.py`
`import proxy`
`proxylist=proxy.get_proxy(n)#n需要自己输入，type是int，n是页数，每页有10条数据，n页就是10n个数据，也可以自己调整`
`#由于爬虫爬下来的代理ip和端口是以字典的形式，所以很难被调用，所以我用遍历将ip和端口一 一 对应写进了两个列表，两个列表由又组成了一个大列表，所以:`
`proxylist=[[ip_1,ip_2],[port_1,port_2]]#举个例子而已，ip_1和port_1是对应的，所以在别的爬虫使用时候可以这样调用:
proxylist[0][0]#第一个ip`
`proxylist[1][0]#第一个ip的port`
